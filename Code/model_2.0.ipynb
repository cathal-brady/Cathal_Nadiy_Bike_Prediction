{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook: Individual Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import holidays\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler, SplineTransformer\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining All Classes We will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        X_copy.reset_index(drop=True)\n",
    "        return X_copy[['counter_id','date', 'site_id','log_bike_count']] \n",
    "\n",
    "class DateFormatter(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        X_copy['date'] = pd.to_datetime(X_copy['date'])\n",
    "        X_copy['year'] = X_copy['date'].dt.year\n",
    "        X_copy['month'] = X_copy['date'].dt.month\n",
    "        #X_copy['week'] = X_copy['date'].dt.isocalendar().week\n",
    "        X_copy['weekday'] = (X_copy['date'].dt.dayofweek + 1)\n",
    "        #X_copy['day'] = X_copy['date'].dt.day\n",
    "        X_copy['hour'] = X_copy['date'].dt.hour\n",
    "        X_copy = X_copy.sort_values('date')\n",
    "        X_copy['track_id'] = X_copy.index\n",
    "        return X_copy\n",
    "\n",
    "\n",
    "class AddRestrictionLevel(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        # Define date ranges and corresponding restriction levels\n",
    "        date_ranges = [\n",
    "            ('16/10/2020', '17/10/2020'),\n",
    "            ('17/10/2020', '28/11/2020'),\n",
    "            ('28/11/2020', '15/12/2020'),\n",
    "            ('15/12/2020', '16/01/2021'),\n",
    "            ('16/01/2021', '19/03/2021'),\n",
    "            ('19/03/2021', '03/05/2021'),\n",
    "            ('03/05/2021', '09/06/2021'),\n",
    "            ('09/06/2021', '20/06/2021'),\n",
    "            ('20/06/2021', '30/06/2021')\n",
    "        ]\n",
    "        \n",
    "        restriction_levels = [3, 5, 4, 2, 1, 5, 4, 2, 1, 0] #85\n",
    "\n",
    "        # Convert date strings to datetime objects\n",
    "        date_ranges = [(pd.to_datetime(start, dayfirst=True), pd.to_datetime(end, dayfirst=True)) for start, end in date_ranges]\n",
    "\n",
    "        # Add restriction_level column based on date ranges\n",
    "        X_copy['restriction_level'] = 0  # Default value\n",
    "        for level, (start_date, end_date) in zip(restriction_levels, date_ranges):\n",
    "            mask = (X_copy['date'] >= start_date) & (X_copy['date'] < end_date)\n",
    "            X_copy.loc[mask, 'restriction_level'] = level\n",
    "\n",
    "        return X_copy\n",
    "\n",
    "class HolidaysFR(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        is_holiday = lambda date: 1 if date in holidays.FR() else 0\n",
    "        is_weekend = lambda day: 1 if day in (6,7) else 0\n",
    "        X_copy = X.copy()\n",
    "        X_copy['is_Holiday'] = X_copy['date'].apply(is_holiday)\n",
    "        X_copy['is_Weekend'] = X_copy['weekday'].apply(is_weekend)\n",
    "        #X_copy.drop(columns='date', inplace=True)\n",
    "        return X_copy\n",
    "\n",
    "class MergeWeatherCovid(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        data = pd.read_csv(os.path.join(\"..\", \"Datasets\", \"weather_data_cleaned.csv\"))\n",
    "        data['date'] = pd.to_datetime(data['date']).astype('datetime64[us]')\n",
    "        merged_data = pd.merge_asof(X, data, on='date')\n",
    "        #merged_data.drop(columns='date', inplace=True)\n",
    "        return merged_data\n",
    "\n",
    "class MergeMultimodal(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        # Import Multimodal Data\n",
    "        mult_df = pd.read_csv(os.path.join(\"..\", \"Datasets\", \"multimodal_data.csv\"))\n",
    "        mult_df['date'] = pd.to_datetime(mult_df['date']).astype('datetime64[us]')\n",
    "        # Averaging and scaling the count\n",
    "        mult_df = pd.DataFrame(mult_df.groupby(['date'])['count'].sum()).reset_index()\n",
    "        scaler = StandardScaler()\n",
    "        numerical_columns = mult_df.select_dtypes(include='number').columns\n",
    "        mult_df[numerical_columns] = scaler.fit_transform(mult_df[numerical_columns])\n",
    "        # Merging data\n",
    "        merged_data = pd.merge_asof(X_copy, mult_df, on='date')\n",
    "        merged_data = merged_data.rename(columns={'count': 'average_multimodal_count'})\n",
    "        #merged_data.drop(columns='date', inplace=True)\n",
    "        return merged_data\n",
    "    \n",
    "class SplitBySite(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        sub_dataframes = []\n",
    "\n",
    "        unique_site_ids = X_copy['site_id'].unique()\n",
    "\n",
    "        for site_id in unique_site_ids:\n",
    "            sub_df = X_copy[X_copy['site_id'] == site_id].copy()\n",
    "            sub_dataframes.append(sub_df)\n",
    "\n",
    "        return sub_dataframes\n",
    "    \n",
    "\n",
    "class EncodeCounter(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        encoded_dataframes = []\n",
    "\n",
    "        for i in range(len(X_copy)):\n",
    "            X_copy[i]['counter_id'] = X_copy[i]['counter_id'].astype('object')\n",
    "            encoded_df = pd.get_dummies(X_copy[i], columns=['counter_id'], dtype=int, drop_first=True)\n",
    "            encoded_dataframes.append(encoded_df)\n",
    "        return encoded_dataframes\n",
    "    \n",
    "\n",
    "class DropOutliers(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=3):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        cleaned_dataframes = []\n",
    "        for i in range(len(X_copy)):\n",
    "            mean_value = X_copy[i]['log_bike_count'].mean()\n",
    "            std_dev = X_copy[i]['log_bike_count'].std()\n",
    "            # #Define a threshold\n",
    "            threshold = 3\n",
    "            # # Identify outliers\n",
    "            outliers = (X_copy[i]['log_bike_count'] - mean_value).abs() > threshold * std_dev\n",
    "            # # Drop outliers\n",
    "            mask = outliers==False\n",
    "            cleaned_dataframes.append(X_copy[i][mask])\n",
    "        return cleaned_dataframes\n",
    "\n",
    "\n",
    "class ModelGen(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model=RandomForestRegressor(n_estimators=200, n_jobs=-1), random_state=42, save_path=(os.path.join(\"..\", \"Trained_Models\"))):\n",
    "        self.model = model\n",
    "        self.random_state = random_state\n",
    "        self.models = []\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.models = []  # Clear previous models\n",
    "        for idx, df in enumerate(X):\n",
    "            X_train = df.drop(columns=['log_bike_count', 'site_id', 'date', 'track_id'], axis=1) \n",
    "            y_train = df['log_bike_count']\n",
    "            model = self.model\n",
    "            model.set_params(random_state=self.random_state)  # Set random state if provided\n",
    "            model.fit(X_train, y_train)\n",
    "            self.models.append(model)\n",
    "\n",
    "            # Save the trained model to the desktop\n",
    "            model_filename = f\"site_ID_{df['site_id'].iloc[0]}_model.joblib\"\n",
    "            model_path = os.path.join(os.path.expanduser(self.save_path), model_filename)\n",
    "            joblib.dump(model, model_path)\n",
    "        return self \n",
    "\n",
    "\n",
    "def add_prediction_column(X):\n",
    "    for df in X:\n",
    "        # Extract the first value of the column 'site_id'\n",
    "        site_id_value = df['site_id'].iloc[0]\n",
    "        #df.drop(columns='log_bike_count', inplace=True)\n",
    "        \n",
    "        # Construct the path for the model file\n",
    "        model_path = os.path.join(\"..\", \"Trained_Models\", f\"site_id_{site_id_value}_model.joblib\")\n",
    "        \n",
    "        # Check if the model file exists\n",
    "        if os.path.exists(model_path):\n",
    "            # Load the model\n",
    "            model = joblib.load(model_path)\n",
    "            # Add a column 'prediction' to the DataFrame with model predictions\n",
    "            df['prediction'] = model.predict(df.drop(columns=['log_bike_count', 'site_id', 'date', 'track_id'], axis=1))\n",
    "            #df.drop('log_bike_count', inplace=True)\n",
    "        else:\n",
    "            print(f\"Model file not found for site_id {site_id_value}\")\n",
    "    out = pd.concat(X, ignore_index=True)\n",
    "    out.drop(columns='log_bike_count', inplace=True)\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "# class ModelGenGridSearch(BaseEstimator, TransformerMixin):\n",
    "#     def __init__(self, model=RandomForestRegressor(n_estimators=200, n_jobs=-1), random_state=42, save_path=(os.path.join(\"..\", \"Trained_Models\"))):\n",
    "#         self.model = model\n",
    "#         self.random_state = random_state\n",
    "#         self.models = []\n",
    "#         self.save_path = save_path\n",
    "\n",
    "#     def fit(self, X, y=None):\n",
    "#         self.models = []  # Clear previous models\n",
    "\n",
    "#         for idx, df in enumerate(X):\n",
    "#             X_train = df.drop(columns=['log_bike_count', 'site_id', 'date', 'track_id'], axis=1) \n",
    "#             y_train = df['log_bike_count']\n",
    "\n",
    "#             # Set random state if provided\n",
    "#             self.model.set_params(random_state=self.random_state)\n",
    "#             print('kfold_')\n",
    "\n",
    "#             # Use 5-fold cross-validation\n",
    "#             kf = KFold(n_splits=5, shuffle=True, random_state=self.random_state)\n",
    "\n",
    "#             # Hyperparameter tuning using GridSearchCV\n",
    "#             param_grid = {\n",
    "#                 'max_depth': [None, 10, 20, 30],\n",
    "#                 'min_samples_split': [2, 5, 10],\n",
    "#                 # 'min_samples_leaf': [1, 2, 4],\n",
    "#                 # 'max_features': ['auto', 'sqrt', 'log2']\n",
    "#             }\n",
    "\n",
    "#             grid_search = GridSearchCV(self.model, param_grid, cv=kf, scoring='neg_mean_squared_error')\n",
    "#             grid_search.fit(X_train, y_train)\n",
    "\n",
    "#             # Get the best model from GridSearchCV\n",
    "#             best_model = grid_search.best_estimator_\n",
    "\n",
    "#             # Save the trained model to the desktop\n",
    "#             model_filename = f\"site_ID_{df['site_id'].iloc[0]}_model.joblib\"\n",
    "#             model_path = os.path.join(os.path.expanduser(self.save_path), model_filename)\n",
    "#             joblib.dump(best_model, model_path)\n",
    "\n",
    "#             self.models.append(best_model)\n",
    "\n",
    "#         return self\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating our pipeline with the following steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessor Pipeline\n",
    "preprocessor = Pipeline([\n",
    "    ('column_selector', ColumnSelector()),\n",
    "    ('date_formatter', DateFormatter()),\n",
    "    ('add_restriction_level', AddRestrictionLevel()),\n",
    "    ('holidays_fr', HolidaysFR()),\n",
    "    ('MergeWeatherCovid', MergeWeatherCovid()),\n",
    "    ('MergeMultimodal', MergeMultimodal())\n",
    "])\n",
    "\n",
    "spliter = Pipeline([\n",
    "    ('SplitBySite', SplitBySite()),\n",
    "    ('EncodeCounter', EncodeCounter()),\n",
    "    ('DropOutliers',DropOutliers())\n",
    "])\n",
    "\n",
    "# Combined Pipeline\n",
    "combined_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('spliter', spliter)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train our models and store them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training set\n",
    "df = pd.read_parquet(os.path.join(\"..\", \"Datasets\", \"train.parquet\"))\n",
    "\n",
    "# Preprocess training set\n",
    "A = combined_pipeline.fit_transform(df)\n",
    "\n",
    "# Train Models\n",
    "ModelGen().fit(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a function to add predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prediction_column(X):\n",
    "    for df in X:\n",
    "        # Extract the first value of the column 'site_id'\n",
    "        site_id_value = df['site_id'].iloc[0]\n",
    "        #df.drop(columns='log_bike_count', inplace=True)\n",
    "        \n",
    "        # Construct the path for the model file\n",
    "        model_path = os.path.join(\"..\", \"Trained_Models\", f\"site_id_{site_id_value}_model.joblib\")\n",
    "        \n",
    "        # Check if the model file exists\n",
    "        if os.path.exists(model_path):\n",
    "            # Load the model\n",
    "            model = joblib.load(model_path)\n",
    "            # Add a column 'prediction' to the DataFrame with model predictions\n",
    "            df['prediction'] = model.predict(df.drop(columns=['log_bike_count', 'site_id', 'date', 'track_id'], axis=1))\n",
    "            #df.drop('log_bike_count', inplace=True)\n",
    "        else:\n",
    "            print(f\"Model file not found for site_id {site_id_value}\")\n",
    "    out = pd.concat(X, ignore_index=True)\n",
    "    out.drop(columns='log_bike_count', inplace=True)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now on full set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Test Set\n",
    "df_test = pd.read_parquet(os.path.join(\"..\", \"Datasets\", \"final_test.parquet\"))\n",
    "df_test['log_bike_count'] = 0\n",
    "\n",
    "# Run Preprocessing Pipeline\n",
    "df_test_preprocessed = combined_pipeline.fit_transform(df_test)\n",
    "df_test_preprocessed = add_prediction_column(df_test_preprocessed)\n",
    "df_sorted = df_test_preprocessed.sort_values(by='track_id')\n",
    "df_sorted.rename(columns={'track_id': 'Id', 'prediction': 'log_bike_count'}, inplace=True)\n",
    "\n",
    "\n",
    "# Extract the selected columns\n",
    "selected_columns = ['Id', 'log_bike_count']\n",
    "result_df = df_sorted[selected_columns]\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "result_df.to_csv(os.path.join(\"..\", \"Code\", \"submission52.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
