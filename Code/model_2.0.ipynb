{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook: Individual Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import holidays\n",
    "from datetime import datetime as dt\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler, SplineTransformer\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining All Classes We will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        X_copy.reset_index(drop=True)\n",
    "        return X_copy[['counter_id','date', 'site_id','log_bike_count']] \n",
    "\n",
    "class DateFormatter(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        X_copy['date'] = pd.to_datetime(X_copy['date'])\n",
    "        X_copy['year'] = X_copy['date'].dt.year\n",
    "        X_copy['month'] = X_copy['date'].dt.month\n",
    "        #X_copy['week'] = X_copy['date'].dt.isocalendar().week\n",
    "        X_copy['weekday'] = (X_copy['date'].dt.dayofweek + 1)\n",
    "        #X_copy['day'] = X_copy['date'].dt.day\n",
    "        X_copy['hr'] = X_copy['date'].dt.hour\n",
    "        X_copy['hr_sin'] = np.sin(X_copy.hr*(2.*np.pi/24))\n",
    "        X_copy['hr_cos'] = np.cos(X_copy.hr*(2.*np.pi/24))\n",
    "        X_copy.drop('hr', axis=1, inplace=True)\n",
    "        X_copy = X_copy.sort_values('date')\n",
    "        X_copy['track_id'] = X_copy.index\n",
    "        return X_copy\n",
    "\n",
    "\n",
    "class AddRestrictionLevel(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        # Define date ranges and corresponding restriction levels\n",
    "        date_ranges = [\n",
    "            ('16/10/2020', '17/10/2020'),\n",
    "            ('17/10/2020', '28/11/2020'),\n",
    "            ('28/11/2020', '15/12/2020'),\n",
    "            ('15/12/2020', '16/01/2021'),\n",
    "            ('16/01/2021', '19/03/2021'),\n",
    "            ('19/03/2021', '03/05/2021'),\n",
    "            ('03/05/2021', '09/06/2021'),\n",
    "            ('09/06/2021', '20/06/2021'),\n",
    "            ('20/06/2021', '30/06/2021')\n",
    "        ]\n",
    "        \n",
    "        restriction_levels = [3, 5, 4, 2, 1, 5, 4, 2, 1, 0] #85\n",
    "\n",
    "        # Convert date strings to datetime objects\n",
    "        date_ranges = [(pd.to_datetime(start, dayfirst=True), pd.to_datetime(end, dayfirst=True)) for start, end in date_ranges]\n",
    "\n",
    "        # Add restriction_level column based on date ranges\n",
    "        X_copy['restriction_level'] = 0  # Default value\n",
    "        for level, (start_date, end_date) in zip(restriction_levels, date_ranges):\n",
    "            mask = (X_copy['date'] >= start_date) & (X_copy['date'] < end_date)\n",
    "            X_copy.loc[mask, 'restriction_level'] = level\n",
    "\n",
    "        return X_copy\n",
    "\n",
    "class HolidaysFR(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        is_holiday = lambda date: 1 if date in holidays.FR() else 0\n",
    "        is_weekend = lambda day: 1 if day in (6,7) else 0\n",
    "        school_holiday  = lambda date, school_hols: 1 if any(start <= date <= end for start, end in school_hols) else 0\n",
    "        \n",
    "        Autumn_20 = (dt(2020, 10, 18), dt(2023, 11, 1))\n",
    "        Xmas_20 = (dt(2020, 12, 20), dt(2021, 1, 3))\n",
    "        Winter_21 = (dt(2021, 2, 14), dt(2021, 2, 28))\n",
    "        Spring_21 = (dt(2021, 4, 11), dt(2021, 4, 25))\n",
    "        Summer_21 = (dt(2021, 7, 7), dt(2021, 9, 7))\n",
    "\n",
    "        school_hols = [Autumn_20, Xmas_20, Winter_21, Spring_21, Summer_21]\n",
    "        \n",
    "        X_copy = X.copy()\n",
    "        X_copy['is_Holiday'] = X_copy['date'].apply(is_holiday)\n",
    "        X_copy['is_Weekend'] = X_copy['weekday'].apply(is_weekend)\n",
    "        X_copy['is_School_Holiday'] = X_copy['date'].apply(lambda date: school_holiday(date, school_hols))\n",
    "        #X_copy.drop(columns='date', inplace=True)\n",
    "        return X_copy\n",
    "\n",
    "class MergeWeatherCovid(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        data = pd.read_csv(os.path.join(\"..\", \"Datasets\", \"weather_data_cleaned.csv\"))\n",
    "        data['date'] = pd.to_datetime(data['date']).astype('datetime64[us]')\n",
    "        merged_data = pd.merge_asof(X, data, on='date')\n",
    "        #merged_data.drop(columns='date', inplace=True)\n",
    "        return merged_data\n",
    "\n",
    "class MergeMultimodal(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        # Import Multimodal Data\n",
    "        mult_df = pd.read_csv(os.path.join(\"..\", \"Datasets\", \"multimodal_data.csv\"))\n",
    "        mult_df['date'] = pd.to_datetime(mult_df['date']).astype('datetime64[us]')\n",
    "        # Averaging and scaling the count\n",
    "        mult_df = pd.DataFrame(mult_df.groupby(['date'])['count'].sum()).reset_index()\n",
    "        scaler = StandardScaler()\n",
    "        numerical_columns = mult_df.select_dtypes(include='number').columns\n",
    "        mult_df[numerical_columns] = scaler.fit_transform(mult_df[numerical_columns])\n",
    "        # Merging data\n",
    "        merged_data = pd.merge_asof(X_copy, mult_df, on='date')\n",
    "        merged_data = merged_data.rename(columns={'count': 'average_multimodal_count'})\n",
    "        #merged_data.drop(columns='date', inplace=True)\n",
    "        return merged_data\n",
    "    \n",
    "class SplitBySite(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        sub_dataframes = []\n",
    "\n",
    "        unique_site_ids = X_copy['site_id'].unique()\n",
    "\n",
    "        for site_id in unique_site_ids:\n",
    "            sub_df = X_copy[X_copy['site_id'] == site_id].copy()\n",
    "            sub_dataframes.append(sub_df)\n",
    "\n",
    "        return sub_dataframes\n",
    "    \n",
    "\n",
    "class EncodeCounter(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        encoded_dataframes = []\n",
    "\n",
    "        for i in range(len(X_copy)):\n",
    "            X_copy[i]['counter_id'] = X_copy[i]['counter_id'].astype('object')\n",
    "            encoded_df = pd.get_dummies(X_copy[i], columns=['counter_id'], dtype=int, drop_first=True)\n",
    "            encoded_dataframes.append(encoded_df)\n",
    "        return encoded_dataframes\n",
    "    \n",
    "\n",
    "class DropOutliers(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=3):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        cleaned_dataframes = []\n",
    "        for i in range(len(X_copy)):\n",
    "            mean_value = X_copy[i]['log_bike_count'].mean()\n",
    "            std_dev = X_copy[i]['log_bike_count'].std()\n",
    "            # #Define a threshold\n",
    "            threshold = 3\n",
    "            # # Identify outliers\n",
    "            outliers = (X_copy[i]['log_bike_count'] - mean_value).abs() > threshold * std_dev\n",
    "            # # Drop outliers\n",
    "            mask = outliers==False\n",
    "            cleaned_dataframes.append(X_copy[i][mask])\n",
    "        return cleaned_dataframes\n",
    "\n",
    "class ModelGen(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model=CatBoostRegressor(iterations=100, depth=5, learning_rate=0.1),\n",
    "                 random_state=42, save_path=(os.path.join(\"..\", \"Trained_Models\"))):\n",
    "        self.model = model\n",
    "        self.random_state = random_state\n",
    "        self.best_model = None\n",
    "        self.best_performance = float('inf')  # or -np.inf if you are maximizing a metric\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.best_model = None  # Clear previous best model\n",
    "        \n",
    "        # Initialize KFold with 10 folds\n",
    "        kf = KFold(n_splits=10, shuffle=True, random_state=self.random_state)\n",
    "        \n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "            model = self.model\n",
    "            model.set_params(random_seed=self.random_state)  # Set random state if provided\n",
    "            model.fit(X_train, y_train, verbose=False)  # You can adjust verbosity as needed\n",
    "\n",
    "            # Evaluate the model using mean squared error\n",
    "            y_pred = model.predict(X_test)\n",
    "            performance = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "            # Update the best model if the current one is better\n",
    "            if performance < self.best_performance:\n",
    "                self.best_model = model\n",
    "                self.best_performance = performance\n",
    "\n",
    "        # Save the best model to the desktop\n",
    "        model_filename = \"model_catboost.joblib\"\n",
    "        model_path = os.path.join(os.path.expanduser(self.save_path), model_filename)\n",
    "        joblib.dump(self.best_model, model_path)\n",
    "\n",
    "        return self\n",
    "\n",
    "def add_prediction_column(X):\n",
    "    for df in X:\n",
    "        # Extract the first value of the column 'site_id'\n",
    "        site_id_value = df['site_id'].iloc[0]\n",
    "        #df.drop(columns='log_bike_count', inplace=True)\n",
    "        \n",
    "        # Construct the path for the model file\n",
    "        model_path = os.path.join(\"..\", \"Trained_Models\", f\"site_id_{site_id_value}_model_catboost.joblib\")\n",
    "        \n",
    "        # Check if the model file exists\n",
    "        if os.path.exists(model_path):\n",
    "            # Load the model\n",
    "            model = joblib.load(model_path)\n",
    "            # Add a column 'prediction' to the DataFrame with model predictions\n",
    "            df['prediction'] = model.predict(df.drop(columns=['log_bike_count', 'site_id', 'date', 'track_id'], axis=1))\n",
    "            #df.drop('log_bike_count', inplace=True)\n",
    "        else:\n",
    "            print(f\"Model file not found for site_id {site_id_value}\")\n",
    "    out = pd.concat(X, ignore_index=True)\n",
    "    out.drop(columns='log_bike_count', inplace=True)\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "# class ModelGenGridSearch(BaseEstimator, TransformerMixin):\n",
    "#     def __init__(self, model=RandomForestRegressor(n_estimators=200, n_jobs=-1), random_state=42, save_path=(os.path.join(\"..\", \"Trained_Models\"))):\n",
    "#         self.model = model\n",
    "#         self.random_state = random_state\n",
    "#         self.models = []\n",
    "#         self.save_path = save_path\n",
    "\n",
    "#     def fit(self, X, y=None):\n",
    "#         self.models = []  # Clear previous models\n",
    "\n",
    "#         for idx, df in enumerate(X):\n",
    "#             X_train = df.drop(columns=['log_bike_count', 'site_id', 'date', 'track_id'], axis=1) \n",
    "#             y_train = df['log_bike_count']\n",
    "\n",
    "#             # Set random state if provided\n",
    "#             self.model.set_params(random_state=self.random_state)\n",
    "#             print('kfold_')\n",
    "\n",
    "#             # Use 5-fold cross-validation\n",
    "#             kf = KFold(n_splits=5, shuffle=True, random_state=self.random_state)\n",
    "\n",
    "#             # Hyperparameter tuning using GridSearchCV\n",
    "#             param_grid = {\n",
    "#                 'max_depth': [None, 10, 20, 30],\n",
    "#                 'min_samples_split': [2, 5, 10],\n",
    "#                 # 'min_samples_leaf': [1, 2, 4],\n",
    "#                 # 'max_features': ['auto', 'sqrt', 'log2']\n",
    "#             }\n",
    "\n",
    "#             grid_search = GridSearchCV(self.model, param_grid, cv=kf, scoring='neg_mean_squared_error')\n",
    "#             grid_search.fit(X_train, y_train)\n",
    "\n",
    "#             # Get the best model from GridSearchCV\n",
    "#             best_model = grid_search.best_estimator_\n",
    "\n",
    "#             # Save the trained model to the desktop\n",
    "#             model_filename = f\"site_ID_{df['site_id'].iloc[0]}_model.joblib\"\n",
    "#             model_path = os.path.join(os.path.expanduser(self.save_path), model_filename)\n",
    "#             joblib.dump(best_model, model_path)\n",
    "\n",
    "#             self.models.append(best_model)\n",
    "\n",
    "#         return self\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating our pipeline with the following steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessor Pipeline\n",
    "preprocessor = Pipeline([\n",
    "    ('column_selector', ColumnSelector()),\n",
    "    ('date_formatter', DateFormatter()),\n",
    "    ('add_restriction_level', AddRestrictionLevel()),\n",
    "    ('holidays_fr', HolidaysFR()),\n",
    "    ('MergeWeatherCovid', MergeWeatherCovid()),\n",
    "    ('MergeMultimodal', MergeMultimodal())\n",
    "])\n",
    "\n",
    "spliter = Pipeline([\n",
    "    ('SplitBySite', SplitBySite()),\n",
    "    ('EncodeCounter', EncodeCounter()),\n",
    "    ('DropOutliers',DropOutliers())\n",
    "])\n",
    "\n",
    "# Combined Pipeline\n",
    "combined_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('spliter', spliter)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train our models and store them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ModelGen()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ModelGen</label><div class=\"sk-toggleable__content\"><pre>ModelGen()</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">model: CatBoostRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;catboost.core.CatBoostRegressor object at 0x00000213D0E2B5D0&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CatBoostRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;catboost.core.CatBoostRegressor object at 0x00000213D0E2B5D0&gt;</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "ModelGen()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import training set\n",
    "df = pd.read_parquet(os.path.join(\"..\", \"Datasets\", \"train.parquet\"))\n",
    "\n",
    "# Preprocess training set\n",
    "A = combined_pipeline.fit_transform(df)\n",
    "\n",
    "# Train Models\n",
    "ModelGen().fit(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a function to add predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prediction_column(X):\n",
    "    for df in X:\n",
    "        # Extract the first value of the column 'site_id'\n",
    "        site_id_value = df['site_id'].iloc[0]\n",
    "        #df.drop(columns='log_bike_count', inplace=True)\n",
    "        \n",
    "        # Construct the path for the model file\n",
    "        model_path = os.path.join(\"..\", \"Trained_Models\", f\"site_id_{site_id_value}_model_catboost.joblib\")\n",
    "        \n",
    "        # Check if the model file exists\n",
    "        if os.path.exists(model_path):\n",
    "            # Load the model\n",
    "            model = joblib.load(model_path)\n",
    "            # Add a column 'prediction' to the DataFrame with model predictions\n",
    "            df['prediction'] = model.predict(df.drop(columns=['log_bike_count', 'site_id', 'date', 'track_id'], axis=1))\n",
    "            #df.drop('log_bike_count', inplace=True)\n",
    "        else:\n",
    "            print(f\"Model file not found for site_id {site_id_value}\")\n",
    "    out = pd.concat(X, ignore_index=True)\n",
    "    out.drop(columns='log_bike_count', inplace=True)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now on full set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Test Set\n",
    "df_test = pd.read_parquet(os.path.join(\"..\", \"Datasets\", \"final_test.parquet\"))\n",
    "df_test['log_bike_count'] = 0\n",
    "\n",
    "# Run Preprocessing Pipeline\n",
    "df_test_preprocessed = combined_pipeline.fit_transform(df_test)\n",
    "df_test_preprocessed = add_prediction_column(df_test_preprocessed)\n",
    "df_sorted = df_test_preprocessed.sort_values(by='track_id')\n",
    "df_sorted.rename(columns={'track_id': 'Id', 'prediction': 'log_bike_count'}, inplace=True)\n",
    "\n",
    "\n",
    "# Extract the selected columns\n",
    "selected_columns = ['Id', 'log_bike_count']\n",
    "result_df = df_sorted[selected_columns]\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "result_df.to_csv(os.path.join(\"..\", \"Code\", \"submission52.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
