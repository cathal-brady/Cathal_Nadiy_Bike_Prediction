{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style = 'darkgrid')\n",
    "\n",
    "# Loading the data\n",
    "meteo_orly = pd.read_csv(os.path.join(\"..\", \"Datasets\", \"meteo_orly.csv\"))\n",
    "meteo_paris = pd.read_csv(os.path.join(\"..\", \"Datasets\", \"meteo_paris.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Date Orly: 2020-09-01 00:00:00\n",
      "Maximum Date Orly: 2021-10-21 12:00:00\n",
      "Minimum Date Paris: 2020-09-01 00:00:00\n",
      "Maximum Date Paris: 2021-11-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Checking Date Range for both Datasets:\n",
    "\n",
    "meteo_orly['date'] = pd.to_datetime(meteo_orly['date'])\n",
    "print(\"Minimum Date Orly:\", meteo_orly['date'].min())\n",
    "print(\"Maximum Date Orly:\", meteo_orly['date'].max())\n",
    "\n",
    "meteo_paris['datetime'] = pd.to_datetime(meteo_paris['datetime'])\n",
    "print(\"Minimum Date Paris:\", meteo_paris['datetime'].min())\n",
    "print(\"Maximum Date Paris:\", meteo_paris['datetime'].max())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be selecting the below features from meteo_paris:\n",
    "- datetime\n",
    "- precip, scaled\n",
    "- visibility, scaled\n",
    "- icon, which is a description of the general weather conditions of the day. (muted for now)\n",
    "\n",
    "<strong>NOTE: AT THIS STAGE WE ARE TAKING DAILY DATA WITHOUT CONSIDERING THE STATION IT COMES FROM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin  \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class ColumnSelectorParis(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_slice = X[['datetime','precip','visibility']] #'icon'\n",
    "        X_slice = X_slice.rename(columns={'precip': 'precipitation', 'datetime': 'date'})\n",
    "        return X_slice\n",
    "\n",
    "class ScaleNumericalParis(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_scaled = X.copy()\n",
    "        scaler = StandardScaler()\n",
    "        numerical_columns = X.select_dtypes(include='number').columns\n",
    "        X_scaled[numerical_columns] = scaler.fit_transform(X[numerical_columns])\n",
    "        return X_scaled\n",
    "\n",
    "paris_preprocess = Pipeline([\n",
    "    (\"ColumnSelectorParis\", ColumnSelectorParis()),\n",
    "    (\"ScaleNumericalParis\", ScaleNumericalParis()),\n",
    "])        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "paris_clean = paris_preprocess.fit_transform(meteo_paris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now tackle the Orly dataset, from we select to following features:\n",
    "- Date\n",
    "- Temperature, scaled\n",
    "\n",
    "To merge the data, we will need this dataset to be sorted by increasing date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnSelectorOrly(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_slice = X[['date','t']]\n",
    "        X_slice = X_slice.rename(columns={'t': 'temperature'})\n",
    "        return X_slice\n",
    "\n",
    "class ScaleNumericalOrly(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_scaled = X.copy()\n",
    "        scaler = StandardScaler()\n",
    "        numerical_columns = X.select_dtypes(include='number').columns\n",
    "        X_scaled[numerical_columns] = scaler.fit_transform(X[numerical_columns])\n",
    "        return X_scaled\n",
    "\n",
    "class SortDateOrly(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.sort_values('date')\n",
    "\n",
    "orly_preprocess = Pipeline([\n",
    "    (\"ColumnSelectorOrly\", ColumnSelectorOrly()),\n",
    "    (\"ScaleNumericalOrly\", ScaleNumericalOrly()),\n",
    "    (\"SortDateOrly\", SortDateOrly())\n",
    "])        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "orly_clean = orly_preprocess.fit_transform(meteo_orly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3322 entries, 3082 to 411\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype         \n",
      "---  ------       --------------  -----         \n",
      " 0   date         3322 non-null   datetime64[ns]\n",
      " 1   temperature  3322 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(1)\n",
      "memory usage: 77.9 KB\n"
     ]
    }
   ],
   "source": [
    "orly_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 456 entries, 0 to 455\n",
      "Data columns (total 3 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   date           456 non-null    datetime64[ns]\n",
      " 1   precipitation  456 non-null    float64       \n",
      " 2   visibility     456 non-null    float64       \n",
      "dtypes: datetime64[ns](1), float64(2)\n",
      "memory usage: 10.8 KB\n"
     ]
    }
   ],
   "source": [
    "paris_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now merge the datasets, and export a csv file that we will use to augment our existing design matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.merge_asof(orly_clean, paris_clean, on='date')\n",
    "merged_data.to_csv(os.path.join(\"..\", \"Datasets\", \"weather_data_cleaned\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we look at Covid-19 cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
